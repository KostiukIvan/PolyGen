import torch
from torch.nn.functional import pad


class VertexTokenizer:
    def __init__(self, padding=True, max_seq_len=2400):
        # NOTE: max_seq_len % reformer__bucket_size == 0 !!!
        self.max_seq_len = max_seq_len
        self.tokens = {
            'bos': torch.tensor([0]),
            'eos': torch.tensor([1]),
            'pad': torch.tensor([2]),
            'ept': torch.tensor([2]),
        }
        self.padd = padding

    def __call__(self, vertices):
        return self.tokenize(vertices)

    def tokenize(self, vertices):
        """
        Input:
        vertices : torch.tensor of shape [:, 3]
        """
        # assert vertices.size(1) == 3 and len(vertices.shape) == 2

        vertices = torch.flatten(vertices)
        vertices = torch.cat([self.tokens['bos'],
                              vertices,
                              self.tokens['eos']])
        axises_tokens = torch.cat([self.tokens['ept'],
                                   torch.arange(len(vertices) - 2) % 3 + 1,
                                   self.tokens['ept']])
        position_tokens = torch.cat([self.tokens['ept'],
                                     torch.arange(len(vertices) - 2) // 3 + 1,
                                     self.tokens['ept']])
        target_vertices = torch.cat([vertices[1:], self.tokens['pad']])
        assert vertices.shape == axises_tokens.shape == position_tokens.shape == target_vertices.shape

        if self.padd:
            padding_len = self.max_seq_len - len(vertices) if self.max_seq_len > len(vertices) else 0
            vertices = pad(vertices, (0, padding_len), value=self.tokens['pad'][0])
            axises_tokens = pad(axises_tokens, (0, padding_len), value=self.tokens['pad'][0])
            position_tokens = pad(position_tokens, (0, padding_len), value=self.tokens['pad'][0])
            target_vertices = pad(target_vertices, (0, padding_len), value=self.tokens['pad'][0])
            # TODO: Add padding mask
            padding_mask = torch.where(vertices == self.tokens['pad'],
                                        torch.ones_like(vertices),
                                        torch.zeros_like(vertices) ).type(torch.bool)
        return {"vertices_tokens": vertices,
                "axises_tokens": axises_tokens,
                "position_tokens": position_tokens,
                "target_vertices": target_vertices,
                "padding_mask": padding_mask}

    def detokenize(self, vertices_reconstruction, seq_len=2400, is_target=False):
        #assert len(vertices_reconstruction.shape) == 2 # only one element could be processed
        # note: Targets needs to be extracted in order to calculate chamfer dist.
        if not is_target:
            vertices_reconstruction = torch.max(vertices_reconstruction, dim=0)[1]

        vertices = torch.reshape(vertices_reconstruction, shape=(-1, 3))
        vertices = vertices.float()
        vertices /= 256
        return vertices

    def get_initial_sampling_tokens(self, num_samples=1):
        """
        Get initial tokens required for generation.
        (Vertices are generated by an autoregressive transformer that's why we need those initial sequences.)
        """
        vertices_tokens = torch.tensor([self.tokens["bos"][0]] * num_samples).unsqueeze(0).reshape(num_samples, 1)
        vertices_tokens = pad(vertices_tokens, (0, self.max_seq_len - 1), value=self.tokens['pad'][0])

        axises_tokens = torch.tensor([self.tokens["ept"][0]] * num_samples).unsqueeze(0).reshape(num_samples, 1)
        axises_tokens = pad(axises_tokens, (0, self.max_seq_len - 1), value=self.tokens['pad'][0])

        position_tokens = torch.tensor([self.tokens["ept"][0]] * num_samples).unsqueeze(0).reshape(num_samples, 1)
        position_tokens = pad(position_tokens, (0, self.max_seq_len - 1), value=self.tokens['pad'][0])

        padding_mask = torch.where(vertices_tokens == self.tokens['pad'],
                                   torch.ones_like(vertices_tokens),
                                   torch.zeros_like(vertices_tokens)).type(torch.bool)

        return {"vertices_tokens": vertices_tokens,
                "axises_tokens": axises_tokens,
                "position_tokens": position_tokens,
                "padding_mask": padding_mask
                }
