import torch
from torch.nn.functional import pad

class VertexTokenizer:
    def __init__(self, padding=True, max_seq_len=2400):
        # NOTE: max_seq_len % reformer__bucket_size == 0 !!!
        self.max_seq_len = max_seq_len
        self.tokens = {
            'bos': torch.tensor([0]),
            'eos': torch.tensor([1]),
            'pad': torch.tensor([2]),
            'ept': torch.tensor([2]),
        }
        self.padd = padding

    def __call__(self, vertices):
        return self.tokenize(vertices)

    def tokenize(self, vertices):
        """
        Input:
        vertices : torch.tensor of shape [:, 3]
        """
        assert vertices.size(1) == 3 and len(vertices.shape) == 2

        vertices = torch.flatten(vertices)
        vertices = torch.cat([self.tokens['bos'], vertices, self.tokens['eos']])
        axises_tokens = torch.cat([self.tokens['ept'], torch.arange(len(vertices) - 2) % 3 + 1, self.tokens['ept']])
        position_tokens = torch.cat([self.tokens['ept'], torch.arange(len(vertices) - 2) // 3 + 1, self.tokens['ept']])
        target_vertices = torch.cat([vertices[1:], self.tokens['pad']])
        assert vertices.shape == axises_tokens.shape == position_tokens.shape == target_vertices.shape

        if self.padd:
            padding_len = self.max_seq_len - len(vertices) if self.max_seq_len > len(vertices) else 0
            vertices = pad(vertices, (0, padding_len), value=self.tokens['pad'][0])
            axises_tokens = pad(axises_tokens, (0, padding_len), value=self.tokens['pad'][0])
            position_tokens = pad(position_tokens, (0, padding_len), value=self.tokens['pad'][0])
            target_vertices = pad(target_vertices, (0, padding_len), value=self.tokens['pad'][0])
            # TODO: Add padding mask

        return {"vertices_tokens": vertices,
                "axises_tokens": axises_tokens,
                "position_tokens": position_tokens,
                "target_vertices": target_vertices}

    def detokenize(self, vertices_reconstruction, seq_len=2400, is_target=False):
        # note: Targets needs to be extracted in order to calculate chamfer dist.
        if not is_target:
            vertices_reconstruction = torch.max(vertices_reconstruction, dim=0)[1]

        vertices = torch.reshape(vertices_reconstruction, shape=(-1, 3))
        vertices = vertices.float()
        vertices /= 256
        return vertices

    def get_initial_sampling_tokens(self, num_samples=1):
        """
        Get initial tokens required for generation.
        (Vertices are generated by an autoregressive transformer that's why we need those initial sequences.)
        """
        vertices_tokens = axises_tokens = position_tokens = torch.tensor([self.tokens["bos"][0]] * num_samples)
        return self.padding(vertices_tokens, axises_tokens, position_tokens)
